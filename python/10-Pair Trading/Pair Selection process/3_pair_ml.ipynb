{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- INSTALLATION AND IMPORTATION ----------\n",
    "\n",
    "!pip install googleapis-common-protos protobuf grpcio pandas systemathics.apis\n",
    "!pip install sklearn\n",
    "!pip install kneed\n",
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from itertools import cycle\n",
    "from sklearn import metrics\n",
    "from kneed import KneeLocator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pair_selection\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pair_selection.Selection(start_date=\"2015-01-01\") # adjustment=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.df_all_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = s.df_all_prices\n",
    "data1 = data\n",
    "data.set_index('Dates', inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('precision', 3)\n",
    "data.describe().T.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate returns and create a data frame\n",
    "returns = data.pct_change().mean()*266\n",
    "returns = pd.DataFrame(returns)\n",
    "returns.columns = ['returns']\n",
    "\n",
    "#Calculate the volatility\n",
    "returns['volatility'] = data.pct_change().std()*np.sqrt(266)\n",
    "\n",
    "data = returns\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the scaler\n",
    "scale = StandardScaler().fit(data)\n",
    "\n",
    "#Fit the scaler\n",
    "scaled_data = pd.DataFrame(scale.fit_transform(data), columns=data.columns, index=data.index)\n",
    "X = scaled_data\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = range(1,15)\n",
    "distortions = []\n",
    "\n",
    "#Fit the method\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters = k)\n",
    "    kmeans.fit(X)\n",
    "    distortions.append(kmeans.inertia_)\n",
    "\n",
    "#Plot the results\n",
    "fig = plt.figure(figsize= (15,5))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('Elbow Method')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl = KneeLocator(K, distortions, curve=\"convex\", direction=\"decreasing\")\n",
    "kl.elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the silhouette method k needs to start from 2\n",
    "K = range(2,15)\n",
    "silhouettes = []\n",
    "\n",
    "#Fit the method\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, init='random')\n",
    "    kmeans.fit(X)\n",
    "    silhouettes.append(silhouette_score(X, kmeans.labels_))\n",
    "\n",
    "#Plot the results\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.plot(K, silhouettes, 'bx-')\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.title('Silhouette Method')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "kl = KneeLocator(K, silhouettes, curve=\"convex\", direction=\"decreasing\")\n",
    "print('Suggested number of clusters: ', kl.elbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 4\n",
    "#Fit the model\n",
    "k_means = KMeans(n_clusters=c)\n",
    "k_means.fit(X)\n",
    "prediction = k_means.predict(X)\n",
    "\n",
    "#Plot the results\n",
    "centroids = k_means.cluster_centers_\n",
    "fig = plt.figure(figsize = (18,10))\n",
    "ax = fig.add_subplot(111)\n",
    "scatter = ax.scatter(X.iloc[:,0],X.iloc[:,1], c=k_means.labels_, cmap=\"rainbow\", label = X.index)\n",
    "ax.set_title('k-Means Cluster Analysis Results')\n",
    "ax.set_xlabel('Mean Return')\n",
    "ax.set_ylabel('Volatility')\n",
    "plt.colorbar(scatter)\n",
    "plt.plot(centroids[:,0],centroids[:,1],'sg',markersize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_series = pd.Series(index=X.index, data=k_means.labels_.flatten())\n",
    "clustered_series_all = pd.Series(index=X.index, data=k_means.labels_.flatten())\n",
    "clustered_series = clustered_series[clustered_series != -1]\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.barh(range(len(clustered_series.value_counts())),clustered_series.value_counts())\n",
    "plt.title('Clusters')\n",
    "plt.xlabel('Stocks per Cluster')\n",
    "plt.ylabel('Cluster Number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))  \n",
    "plt.title(\"Dendrograms\")  \n",
    "dend = shc.dendrogram(shc.linkage(X, method='ward'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))  \n",
    "plt.title(\"Dendrogram\")  \n",
    "dend = shc.dendrogram(shc.linkage(X, method='ward'))\n",
    "plt.axhline(y=13.5, color='purple', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model\n",
    "clusters = 4\n",
    "hc = AgglomerativeClustering(n_clusters= clusters, affinity='euclidean', linkage='ward')\n",
    "labels = hc.fit_predict(X)\n",
    "\n",
    "#Plot the results\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "scatter = ax.scatter(X.iloc[:,0], X.iloc[:,1], c=labels, cmap='rainbow')\n",
    "ax.set_title('Hierarchical Clustering Results')\n",
    "ax.set_xlabel('Mean Return')\n",
    "ax.set_ylabel('Volatility')\n",
    "plt.colorbar(scatter)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Affinity Propagation Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = AffinityPropagation()\n",
    "ap.fit(X)\n",
    "labels1 = ap.predict(X)\n",
    "\n",
    "#Plot the results\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "scatter = ax.scatter(X.iloc[:,0], X.iloc[:,1], c=labels1, cmap='rainbow')\n",
    "ax.set_title('Affinity Propagation Clustering Results')\n",
    "ax.set_xlabel('Mean Return')\n",
    "ax.set_ylabel('Volatility')\n",
    "plt.colorbar(scatter)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the cluster centers and labels\n",
    "cci = ap.cluster_centers_indices_\n",
    "labels2 = ap.labels_\n",
    "\n",
    "#Print their number\n",
    "clusters = len(cci)\n",
    "print('The number of clusters is:',clusters)\n",
    "\n",
    "#Plot the results\n",
    "X_ap = np.asarray(X)\n",
    "plt.close('all')\n",
    "plt.figure(1)\n",
    "plt.clf\n",
    "fig=plt.figure(figsize=(15,10))\n",
    "colors = cycle('cmykrgbcmykrgbcmykrgbcmykrgb')\n",
    "for k, col in zip(range(clusters),colors):\n",
    "    cluster_members = labels2 == k\n",
    "    cluster_center = X_ap[cci[k]]\n",
    "    plt.plot(X_ap[cluster_members, 0], X_ap[cluster_members, 1], col + '.')\n",
    "    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col, markeredgecolor='k', markersize=12)\n",
    "    for x in X_ap[cluster_members]:\n",
    "        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare clustering models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"k-Means Clustering\", metrics.silhouette_score(X, k_means.labels_, metric='euclidean'))\n",
    "print(\"Hierarchical Clustering\", metrics.silhouette_score(X, hc.fit_predict(X), metric='euclidean'))\n",
    "print(\"Affinity Propagation Clustering\", metrics.silhouette_score(X, ap.labels_, metric='euclidean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the trading pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_size_limit = 1000\n",
    "counts = clustered_series.value_counts()\n",
    "ticker_count = counts[(counts>1) & (counts<=cluster_size_limit)]\n",
    "print (\"Number of clusters: %d\" % len(ticker_count))\n",
    "print (\"Number of Pairs: %d\" % (ticker_count*(ticker_count-1)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cointegrated_pairs(data, significance=0.05):\n",
    "    n = data.shape[1]    \n",
    "    score_matrix = np.zeros((n, n))\n",
    "    pvalue_matrix = np.ones((n, n))\n",
    "    keys = data.keys()\n",
    "    pairs = []\n",
    "    for i in range(1):\n",
    "        for j in range(i+1, n):\n",
    "            S1 = data[keys[i]]            \n",
    "            S2 = data[keys[j]]\n",
    "            result = coint(S1, S2)\n",
    "            score = result[0]\n",
    "            pvalue = result[1]\n",
    "            score_matrix[i, j] = score\n",
    "            pvalue_matrix[i, j] = pvalue\n",
    "            if pvalue < significance:\n",
    "                pairs.append((keys[i], keys[j]))\n",
    "    return score_matrix, pvalue_matrix, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import coint\n",
    "\n",
    "cluster_dict = {}\n",
    "\n",
    "for i, clust in enumerate(ticker_count.index):\n",
    "    tickers = clustered_series[clustered_series == clust].index\n",
    "    score_matrix, pvalue_matrix, pairs = find_cointegrated_pairs(data1[tickers])\n",
    "    cluster_dict[clust] = {}\n",
    "    cluster_dict[clust]['score_matrix'] = score_matrix\n",
    "    cluster_dict[clust]['pvalue_matrix'] = pvalue_matrix\n",
    "    cluster_dict[clust]['pairs'] = pairs\n",
    "    \n",
    "pairs = []   \n",
    "for cluster in cluster_dict.keys():\n",
    "    pairs.extend(cluster_dict[cluster]['pairs'])\n",
    "    \n",
    "print (\"Number of pairs:\", len(pairs))\n",
    "print (\"In those pairs, we found %d unique tickers.\" % len(np.unique(pairs)))\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get best pairs function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import coint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pair_selection.Selection(start_date=\"2015-01-01\") # adjustment=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_pairs(df_prices, start_date='', end_date=''):\n",
    "    # Retrieve only the prices between the two specified dates\n",
    "    mask = (df_prices['Dates'] >= start_date) & (df_prices['Dates'] <= end_date)\n",
    "    data = df_prices.loc[mask]\n",
    "    \n",
    "    data.set_index('Dates', inplace=True)\n",
    "    data1 = data.copy(deep=True)\n",
    "    \n",
    "    #Calculate returns and create a data frame\n",
    "    returns = data.pct_change().mean()*266\n",
    "    returns = pd.DataFrame(returns)\n",
    "    returns.columns = ['returns']\n",
    "\n",
    "    #Calculate the volatility\n",
    "    returns['volatility'] = data.pct_change().std()*np.sqrt(266)\n",
    "\n",
    "    data = returns\n",
    "    \n",
    "    #Prepare the scaler\n",
    "    scale = StandardScaler().fit(data)\n",
    "\n",
    "    #Fit the scaler\n",
    "    scaled_data = pd.DataFrame(scale.fit_transform(data), columns=data.columns, index=data.index)\n",
    "    X = scaled_data\n",
    "    \n",
    "    K = range(1,15)\n",
    "    distortions = []\n",
    "\n",
    "    #Fit the method\n",
    "    for k in K:\n",
    "        kmeans = KMeans(n_clusters = k)\n",
    "        kmeans.fit(X)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "    \n",
    "    kl = KneeLocator(K, distortions, curve=\"convex\", direction=\"decreasing\")\n",
    "    c = kl.elbow\n",
    "\n",
    "    #Fit the model\n",
    "    k_means = KMeans(n_clusters=c)\n",
    "    k_means.fit(X)\n",
    "    prediction = k_means.predict(X)\n",
    "    \n",
    "    clustered_series = pd.Series(index=X.index, data=k_means.labels_.flatten())\n",
    "    clustered_series_all = pd.Series(index=X.index, data=k_means.labels_.flatten())\n",
    "    clustered_series = clustered_series[clustered_series != -1]\n",
    "    \n",
    "    cluster_size_limit = 1000\n",
    "    counts = clustered_series.value_counts()\n",
    "    ticker_count = counts[(counts>1) & (counts<=cluster_size_limit)]\n",
    "    \n",
    "    cluster_dict = {}\n",
    "\n",
    "    for i, clust in enumerate(ticker_count.index):\n",
    "        tickers = clustered_series[clustered_series == clust].index\n",
    "        score_matrix, pvalue_matrix, pairs = find_cointegrated_pairs(data1[tickers])\n",
    "        cluster_dict[clust] = {}\n",
    "        cluster_dict[clust]['score_matrix'] = score_matrix\n",
    "        cluster_dict[clust]['pvalue_matrix'] = pvalue_matrix\n",
    "        cluster_dict[clust]['pairs'] = pairs\n",
    "\n",
    "    pairs = []   \n",
    "    for cluster in cluster_dict.keys():\n",
    "        pairs.extend(cluster_dict[cluster]['pairs'])\n",
    "\n",
    "    # print (\"Number of pairs:\", len(pairs))\n",
    "    # print (\"In those pairs, we found %d unique tickers.\" % len(np.unique(pairs)))\n",
    "    # print(pairs)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = s.df_all_prices\n",
    "top = get_best_pairs(prices, \"2015-01-01\", \"2022-01-23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [[top[i][0], top[i][1]] for i in range(len(top))]\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all time best pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import coint\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime, date\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date=\"2015-01-01\"\n",
    "interval=6\n",
    "repetition=1\n",
    "filename=\"ml_best_pairs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pair_selection.Selection(start_date=start_date) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_months(start_date, interval):\n",
    "    \"\"\"From a starting date in string format 'YYYY-MM-DD', return the same format date after an interval of X month(s) later.\"\"\"\n",
    "    date_format = '%Y-%m-%d'\n",
    "    dtObj = datetime.strptime(start_date, date_format)\n",
    "    # Add months to a given datetime object\n",
    "    future_date = dtObj + relativedelta(months=interval)\n",
    "    # Convert datetime object to string in required format\n",
    "    future_date_str = future_date.strftime(date_format)\n",
    "    return future_date_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = add_months(start_date, interval)\n",
    "prices = s.df_all_prices\n",
    "\n",
    "# To know when to stop the loop: the month and year of the last saved price\n",
    "last_date = prices.iloc[-1]['Dates'] \n",
    "last_year = last_date.strftime('%Y')\n",
    "last_month = last_date.strftime('%m')\n",
    "\n",
    "best_pairs_dict = {}\n",
    "while True:\n",
    "    # Get the best pairs of a specific period\n",
    "    top = get_best_pairs(prices, start_date, end_date)\n",
    "    best_pairs_dict[end_date] = [[top[i][0], top[i][1]] for i in range(len(top))]\n",
    "    # increment start and end for the get_best_pairs() computation\n",
    "    start_date = add_months(start_date, repetition)\n",
    "    end_date = add_months(end_date, repetition)\n",
    "    # check if we've reached the month and year of the last saved price\n",
    "    splt = end_date.split('-')    \n",
    "    if splt[0] == last_year and splt[1] == last_month:\n",
    "        break\n",
    "\n",
    "# Save the results in a json for backtesting purpose\n",
    "with open(filename + \".json\", \"w\") as f:\n",
    "    json.dump(best_pairs_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
